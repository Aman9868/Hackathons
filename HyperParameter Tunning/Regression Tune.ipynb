{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIbrary###\n",
    "#LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import r2_score,roc_auc_score,classification_report,mean_squared_error,accuracy_score,confusion_matrix,precision_score,recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIGHT GBM#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########TUNNING PARAMTERES FOR REGRESSSION MODELS USING RANDOMISED SEARCH CV ########\n",
    "\n",
    "#1.LIGHT GBM\n",
    "\n",
    "lgbm = LGBMRegressor()\n",
    "lgb_grid = {\n",
    "    'n_estimators': [100, 200, 400, 500],\n",
    "    'colsample_bytree': [0.9, 1.0],\n",
    "    'max_depth': [5,10,15,20,25,35,None],\n",
    "    'num_leaves': [20, 30, 50, 100],\n",
    "    'reg_alpha': [1.0, 1.1, 1.2, 1.3],\n",
    "    'reg_lambda': [1.0, 1.1, 1.2, 1.3],\n",
    "    'min_split_gain': [0.2, 0.3, 0.4],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'learning_rate': [0.05, 0.1]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(lgbm,lgb_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 100)\n",
    "search.fit(X,y)\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_estimator_)\n",
    "print(search.cv_results_)\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.RANDOMFOREST\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "grid = {'n_estimators' : [100,200,500,800,1000,1200],\n",
    "           'max_depth' : [3,5,7,10,15,25,40,None],\n",
    "           'min_samples_split':[2,4,6,10],\n",
    "           'min_samples_leaf':[2,4,6,8]   \n",
    "           }\n",
    "\n",
    "search = RandomizedSearchCV(rf,grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 50)\n",
    "search.fit(X,y)\n",
    "\n",
    "\n",
    "print(search.best_params_)\n",
    "print(search.best_estimator_)\n",
    "print(search.cv_results_)\n",
    "print(search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.SVM\n",
    "from sklearn.svm import SVR\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf','poly','linear']}  \n",
    "  \n",
    "\n",
    "sv = SVR()\n",
    "\n",
    "    \n",
    "grid = GridSearchCV(sv,param_grid,\n",
    "                           scoring = 'neg_mean_squared_error',\n",
    "                           cv = 10,\n",
    "                           n_jobs = -1)\n",
    "grid.fit(X_train, y_train)\n",
    "grid.best_params_\n",
    "print(grid.best_params_)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBM#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.GBM\n",
    "\n",
    "gbmr = GradientBoostingRegressor()\n",
    "gb_grid = {\n",
    "    'n_estimators'     : range(100,1000,100),\n",
    "    'max_depth'        : [5,10,15,20,25,35,None],\n",
    "    'loss'             :['ls','lad','huber','quantile'],\n",
    "    'subsample'        : [0.8, 0.9, 1.0],\n",
    "    'min_samples_leaf' : [1,2,5,10],\n",
    "    'min_samples_split': [2,5,10,15,100],\n",
    "    'learning_rate'    : [0.1,0.03,0.4,0.5,0.7]\n",
    "\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(gbmr,gb_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=-1, n_iter = 100)\n",
    "search.fit(x_train,y_train)\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.XG BOOST\n",
    "\n",
    "## Hyper Parameter Optimization\n",
    "regressor = XGBRegressor()\n",
    "booster=['gbtree','gblinear']\n",
    "base_score=[0.25,0.5,0.75,1]\n",
    "n_estimators = [100, 500, 900, 1100, 1500]\n",
    "max_depth = [2, 3, 5, 10, 15]\n",
    "booster=['gbtree','gblinear']\n",
    "learning_rate=[0.05,0.1,0.15,0.20]\n",
    "min_child_weight=[1,2,3,4]\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth':max_depth,\n",
    "    'learning_rate':learning_rate,\n",
    "    'min_child_weight':min_child_weight,\n",
    "    'booster':booster,\n",
    "    'base_score':base_score\n",
    "    }\n",
    "\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cv = RandomizedSearchCV(estimator=regressor,\n",
    "            param_distributions=hyperparameter_grid,\n",
    "            cv=5, n_iter=50,\n",
    "            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)\n",
    "\n",
    "random_cv.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELASTIC NET#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use grid search to tune the parameters:\n",
    "\n",
    "parametersGrid = {\"max_iter\": [10,100,1000,10000],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": [0, 0.25, 0.5, 0.75, 1.0]}\n",
    "\n",
    "eNet = ElasticNet()\n",
    "grid = GridSearchCV(eNet, parametersGrid, scoring='neg_root_mean_squared_error', cv=10 ,refit='True', verbose = 10, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_) \n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RIDGE REGRESSION#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ridge=Ridge()\n",
    "parameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "# Set up the random search with 4-fold cross validation\n",
    "random_cvs = RandomizedSearchCV(estimator=ridge,\n",
    "            param_distributions=parameters,\n",
    "            cv=cv, n_iter=50,\n",
    "            scoring = 'neg_mean_squared_error',n_jobs = -1,\n",
    "            verbose = 5, \n",
    "            return_train_score = True,\n",
    "            random_state=42)\n",
    "\n",
    "random_cvs.fit(x_train,y_train)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (random_cvs.best_score_, random_cvs.best_params_))\n",
    "means = random_cvs.cv_results_['mean_test_score']\n",
    "stds = random_cvs.cv_results_['std_test_score']\n",
    "params = random_cvs.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
